{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a21d5d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for llm\n",
    "import getpass\n",
    "import os\n",
    "import langchain\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import tensorflow\n",
    "from datasets import load_dataset\n",
    "import tqdm as notebook_tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d8c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "id": "850389e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key: Google Gemini\n",
    "# AIzaSyDgylN-Pss_RgB5Or4OO4GCTOSgPVVpg5g\n",
    "\n",
    "# API Key: Personal Access Token\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
=======
>>>>>>> 9a1dfd5 (Removed secretS)
   "execution_count": 5,
   "id": "7f36b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Model getting\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af245d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting model to train\n",
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b29e505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Test to see what we're training\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cb74dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Reddit Data # This can be optimised hella since I know where the files are\n",
    "# This function will load all the reddit posts and comments from the specified directory\n",
    "def load_reddit_data(directory=\".\"):\n",
    "    posts_data = []\n",
    "    comments_data = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if filename.startswith(\"reddit_posts_\") and filename.endswith(\".csv\"):\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                if not df.empty:\n",
    "                    posts_data.append(df)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"Skipping empty or malformed file: {filename}\")\n",
    "        elif filename.startswith(\"reddit_comments_\") and filename.endswith(\".csv\"):\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                if not df.empty:\n",
    "                    comments_data.append(df)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"Skipping empty or malformed file: {filename}\")\n",
    "\n",
    "    all_posts = pd.concat(posts_data, ignore_index=True) if posts_data else pd.DataFrame()\n",
    "    all_comments = pd.concat(comments_data, ignore_index=True) if comments_data else pd.DataFrame()\n",
    "\n",
    "    return all_posts, all_comments\n",
    "# Explanation of relevant columns for prompts and responses:\n",
    "# For posts, 'title' and 'selftext' columns are likely candidates for prompts.\n",
    "# 'selftext' contains the main body of the post, which can be a detailed story prompt.\n",
    "# 'title' can serve as a shorter prompt or a story idea.\n",
    "# For comments, 'comment_body' is the content that will be used as the response to a prompt.\n",
    "# The 'post_id' column in both dataframes will be crucial for linking posts to their respective comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fd98052",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df, comments_df = load_reddit_data(\"../data/raw/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87d35e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all the reddit data\n",
    "posts_df = posts_df.drop_duplicates(subset=['id'])\n",
    "posts_df = posts_df[['id', 'title', 'num_comments', 'score']].rename(columns={'id': 'post_id'})\n",
    "\n",
    "comments_df = comments_df.drop_duplicates(subset=['comment_id'])\n",
    "comments_df = comments_df[['comment_id', 'post_id', 'comment_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f27aafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts DataFrame Head:\n",
      "   post_id                                              title  num_comments  \\\n",
      "0  1lk2jzm  [WP] \"You can't quit! I'm the only one that ca...             7   \n",
      "1  1lk4xdj  [WP] Your greed demon roommate Offered you the...             2   \n",
      "2  1ljtyzu  [WP] A time traveler arrives from the future t...             4   \n",
      "3  1ljpk06  [WP] \"The super soldier serum works perfectly,...            20   \n",
      "4  1lk04u1  [WP] You die unexpectedly. You encounter a dei...             5   \n",
      "\n",
      "   score  \n",
      "0    NaN  \n",
      "1    NaN  \n",
      "2    NaN  \n",
      "3    NaN  \n",
      "4    NaN  \n",
      "\n",
      "Comments DataFrame Head:\n",
      "  comment_id  post_id                                       comment_body\n",
      "0    myzcohn  1lgtwz8  It felt wrong. The air was so dirty, it felt l...\n",
      "1    myyyvg1  1lgtwz8  Okay, that works, which is bad.  \\nI try sever...\n",
      "2    myzb5sw  1lgtwz8  No one had noticed the magic that appeared, to...\n",
      "3    myz6a0x  1lgtwz8  Well, if that works, then-\\n\\nI open a portal ...\n",
      "4    myzcch6  1lgtwz8  I was noting down the most outstanding reading...\n",
      "\n",
      "Posts DataFrame Columns:\n",
      "Index(['post_id', 'title', 'num_comments', 'score'], dtype='object')\n",
      "\n",
      "Comments DataFrame Columns:\n",
      "Index(['comment_id', 'post_id', 'comment_body'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Posts DataFrame Head:\")\n",
    "print(posts_df.head())\n",
    "print(\"\\nComments DataFrame Head:\")\n",
    "print(comments_df.head())\n",
    "\n",
    "print(\"\\nPosts DataFrame Columns:\")\n",
    "print(posts_df.columns)\n",
    "print(\"\\nComments DataFrame Columns:\")\n",
    "print(comments_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b33b5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LangChain and Gemini setup code added. Remember to set your GOOGLE_API_KEY environment variable.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ubuntu/upload'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_df, val_df\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m     posts_df, comments_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_reddit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/ubuntu/upload\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Data Preprocessing ---\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     65\u001b[0m     prompt_response_pairs \u001b[38;5;241m=\u001b[39m create_prompt_response_pairs(posts_df, comments_df)\n",
      "Cell \u001b[0;32mIn[33], line 7\u001b[0m, in \u001b[0;36mload_reddit_data\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      4\u001b[0m posts_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m comments_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      8\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, filename)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreddit_posts_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ubuntu/upload'"
     ]
    }
   ],
   "source": [
    "# Next, we will proceed with data preprocessing to create prompt-response pairs.\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# --- LangChain and Gemini Setup ---\n",
    "# To use Gemini with LangChain, you need to configure your Google API key.\n",
    "# It's recommended to load this from an environment variable for security.\n",
    "# Replace 'YOUR_API_KEY' with your actual Gemini API key.\n",
    "# You can get an API key from Google AI Studio: https://aistudio.google.com/app/apikey\n",
    "\n",
    "# Set your API key as an environment variable (e.g., GOOGLE_API_KEY)\n",
    "# For example, in your terminal before running the script:\n",
    "# export GOOGLE_API_KEY='YOUR_API_KEY'\n",
    "\n",
    "# Or, you can directly set it in the script (less secure for production):\n",
    "# os.environ['GOOGLE_API_KEY'] = 'YOUR_API_KEY'\n",
    "\n",
    "genai.configure(api_key=os.environ.get('GOOGLE_API_KEY'))\n",
    "\n",
    "print(\"\\nLangChain and Gemini setup code added. Remember to set your GOOGLE_API_KEY environment variable.\")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\[\\]]', '', text)  # Remove special characters except common punctuation\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = text.strip()  # Remove leading/trailing whitespace\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "def create_prompt_response_pairs(posts_df, comments_df):\n",
    "    # Merge posts and comments on 'id' from posts and 'post_id' from comments\n",
    "    # We want to link each post (prompt) to its comments (responses)\n",
    "    merged_df = pd.merge(posts_df, comments_df, left_on='id', right_on='post_id', how='inner')\n",
    "\n",
    "    # Combine title and selftext for the prompt\n",
    "    # Fill NaN in 'selftext' with an empty string so it can be concatenated\n",
    "    merged_df['full_prompt'] = merged_df['title'].fillna('') + \" \" + merged_df['selftext'].fillna('')\n",
    "    merged_df['full_prompt'] = merged_df['full_prompt'].apply(clean_text)\n",
    "    merged_df['comment_body'] = merged_df['comment_body'].apply(clean_text)\n",
    "\n",
    "    # Filter out empty prompts or responses after cleaning\n",
    "    merged_df = merged_df[merged_df['full_prompt'].str.strip() != '']\n",
    "    merged_df = merged_df[merged_df['comment_body'].str.strip() != '']\n",
    "\n",
    "    # For story generation, we can consider each post as a prompt and its comments as potential continuations/responses.\n",
    "    # We'll create a dataset where each row is a (prompt, response) pair.\n",
    "    # A post can have multiple comments, so we'll have multiple entries for a single post.\n",
    "    prompt_response_data = merged_df[['full_prompt', 'comment_body']].rename(columns={'comment_body': 'response'})\n",
    "\n",
    "    return prompt_response_data\n",
    "\n",
    "def split_data(data_df, test_size=0.2, random_state=42):\n",
    "    train_df, val_df = train_test_split(data_df, test_size=test_size, random_state=random_state)\n",
    "    return train_df, val_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    posts_df, comments_df = load_reddit_data('/home/ubuntu/upload')\n",
    "\n",
    "    print('\\n--- Data Preprocessing ---')\n",
    "    prompt_response_pairs = create_prompt_response_pairs(posts_df, comments_df)\n",
    "    print(f'Total prompt-response pairs: {len(prompt_response_pairs)}')\n",
    "    print('Prompt-Response Pairs Head:')\n",
    "    print(prompt_response_pairs.head())\n",
    "\n",
    "    train_data, val_data = split_data(prompt_response_pairs)\n",
    "    print(f'Training data size: {len(train_data)}')\n",
    "    print(f'Validation data size: {len(val_data)}')\n",
    "\n",
    "    # Save processed data for later use\n",
    "    train_data.to_csv('train_data.csv', index=False)\n",
    "    val_data.to_csv('val_data.csv', index=False)\n",
    "    print('\\nProcessed data saved to train_data.csv and val_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# --- Fine-tuning Pipeline (Conceptual) ---\n",
    "# Direct fine-tuning of Gemini models is typically done through Google Cloud Vertex AI.\n",
    "# LangChain provides an excellent framework for interacting with LLMs, including Gemini,\n",
    "# and for managing prompts and chains. While LangChain doesn't directly offer a 'fine-tune'\n",
    "# function for Gemini models in the same way you might fine-tune a smaller open-source model,\n",
    "# it can be used to prepare data for fine-tuning on Vertex AI, or to implement few-shot learning\n",
    "# with your processed data.\n",
    "\n",
    "# For the purpose of demonstrating the pipeline, we will show how to set up a LangChain\n",
    "# interaction with a Gemini model for story generation, and how you would conceptually\n",
    "# integrate your processed data.\n",
    "\n",
    "# 1. Initialize the Gemini Model\n",
    "# You can specify the model name, e.g., 'gemini-pro'.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)\n",
    "\n",
    "# 2. Define the Prompt Template\n",
    "# This template will guide the model in generating short stories based on your prompts.\n",
    "# We'll use the 'full_prompt' from your Reddit data as the input.\n",
    "story_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"prompt\"],\n",
    "    template=\"\"\"You are a creative short story writer. Based on the following prompt, write a compelling short story:\n",
    "\n",
    "Prompt: {prompt}\n",
    "\n",
    "Story:\"\"\"\n",
    ")\n",
    "\n",
    "# 3. Create an LLM Chain\n",
    "# This chain combines the prompt template with the Gemini LLM.\n",
    "story_chain = LLMChain(llm=llm, prompt=story_prompt_template)\n",
    "\n",
    "# --- How to use your processed data for 'training' or few-shot learning ---\n",
    "# Since direct fine-tuning is via Vertex AI, here's how you'd use your data:\n",
    "\n",
    "# Option A: Few-Shot Learning with LangChain (if your data is small enough)\n",
    "# You can create examples from your train_data.csv and include them in your prompt.\n",
    "# This is effective for smaller datasets or for guiding the model's style.\n",
    "# Example (this would be done within a more complex prompt engineering setup):\n",
    "# from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "# from langchain.chains import LLMChain\n",
    "\n",
    "# examples = []\n",
    "# # Load a few examples from your train_data.csv\n",
    "# # For demonstration, let's assume you have a list of dicts like:\n",
    "# # examples = [\n",
    "# #     {\"prompt\": \"A lone astronaut discovers a strange artifact on Mars.\", \"response\": \"The artifact pulsed with an eerie light...\"},\n",
    "# #     # ... more examples\n",
    "# # ]\n",
    "\n",
    "# # example_prompt = PromptTemplate(input_variables=[\"prompt\", \"response\"], template=\"Prompt: {prompt}\\nStory: {response}\")\n",
    "# # few_shot_prompt = FewShotPromptTemplate(\n",
    "# #     examples=examples,\n",
    "# #     example_prompt=example_prompt,\n",
    "# #     prefix=\"Write a short story based on the given prompt. Here are some examples:\",\n",
    "# #     suffix=\"\\nPrompt: {prompt}\\nStory:\",\n",
    "# #     input_variables=[\"prompt\"],\n",
    "# #     example_separator=\"\\n\\n\"\n",
    "# # )\n",
    "\n",
    "# # few_shot_chain = LLMChain(llm=llm, prompt=few_shot_prompt)\n",
    "\n",
    "# Option B: Preparing Data for Vertex AI Fine-tuning\n",
    "# Your `train_data.csv` and `val_data.csv` are already in a suitable format (prompt, response pairs).\n",
    "# You would typically upload these to Google Cloud Storage and then use Vertex AI's API or UI\n",
    "# to initiate a fine-tuning job. The `full_prompt` column would be your input, and `response`\n",
    "# would be your target output for the model to learn from.\n",
    "\n",
    "# Example of how you would use the chain for inference after hypothetical fine-tuning or with few-shot:\n",
    "if __name__ == '__main__':\n",
    "    # ... (previous data loading and preprocessing code)\n",
    "\n",
    "    # Example of generating a story using the defined chain\n",
    "    sample_prompt = \"A detective investigates a mysterious disappearance in a foggy, old town.\"\n",
    "    print(f\"\\n--- Generating Story for Prompt: {sample_prompt} ---\")\n",
    "    # For actual generation, you would call:\n",
    "    # generated_story = story_chain.run(prompt=sample_prompt)\n",
    "    # print(generated_story)\n",
    "    print(\"To generate a story, uncomment the 'story_chain.run' line and ensure GOOGLE_API_KEY is set.\")\n",
    "    print(\"Note: This is a conceptual demonstration. Actual fine-tuning of Gemini models is done via Google Cloud Vertex AI.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af604e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "\n",
    "# --- Model Testing and Validation (Conceptual) ---\n",
    "# Since direct fine-tuning is handled by Vertex AI, the evaluation metrics\n",
    "# for the fine-tuned model would typically be provided by Vertex AI itself.\n",
    "# However, we can demonstrate how you would conceptually load a model (or use the base model)\n",
    "# and perform inference for validation or testing purposes.\n",
    "\n",
    "# For evaluating a generative model like Gemini for short story generation,\n",
    "# traditional metrics (like accuracy) are not directly applicable.\n",
    "# Instead, human evaluation or metrics like ROUGE, BLEU (for text similarity),\n",
    "# or more advanced metrics that assess coherence, creativity, and relevance\n",
    "# would be used. LangChain can help with setting up the inference pipeline.\n",
    "\n",
    "# 1. Load the (hypothetically) fine-tuned model or use the base model\n",
    "# In a real scenario with Vertex AI, you would deploy your fine-tuned model\n",
    "# and then interact with its endpoint. Here, we continue with the base Gemini model.\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7) # Already initialized above\n",
    "\n",
    "# 2. Generate Inferences on Validation Data\n",
    "# We will take a few samples from the validation set and generate stories.\n",
    "if __name__ == '__main__':\n",
    "    # ... (previous data loading, preprocessing, and fine-tuning conceptual code)\n",
    "\n",
    "    print(\"\\n--- Model Testing and Validation ---\")\n",
    "    # Load validation data (assuming it was saved)\n",
    "    try:\n",
    "        val_data = pd.read_csv(\"val_data.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Validation data (val_data.csv) not found. Please run the data preprocessing step first.\")\n",
    "        val_data = pd.DataFrame()\n",
    "\n",
    "    if not val_data.empty:\n",
    "        print(\"Generating sample stories from validation prompts...\")\n",
    "        sample_val_prompts = val_data[\"full_prompt\"].sample(min(5, len(val_data)), random_state=42).tolist()\n",
    "\n",
    "        for i, prompt in enumerate(sample_val_prompts):\n",
    "            print(f\"\\nSample {i+1} Prompt: {prompt}\")\n",
    "            # In a real scenario, you would call the model here:\n",
    "            # generated_story = story_chain.run(prompt=prompt)\n",
    "            # print(f\"Generated Story: {generated_story}\")\n",
    "            print(\"Generated Story: [Story generation requires GOOGLE_API_KEY and actual model inference.]\")\n",
    "\n",
    "    print(\"\\nConceptual code for model testing and validation added. Actual evaluation would involve human review or advanced NLP metrics.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
