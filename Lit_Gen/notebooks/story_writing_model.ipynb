{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a21d5d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for llm\n",
    "import getpass\n",
    "import os\n",
    "import langchain\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import google.generativeai as genai\n",
    "import tensorflow\n",
    "from datasets import load_dataset\n",
    "import tqdm as notebook_tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d8c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f36b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Model getting\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af245d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting model to train\n",
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b29e505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Test to see what we're training\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb74dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Reddit Data # This can be optimised hella since I know where the files are\n",
    "# This function will load all the reddit posts and comments from the specified directory\n",
    "def load_reddit_data(directory=\".\"):\n",
    "    posts_data = []\n",
    "    comments_data = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if filename.startswith(\"reddit_posts_\") and filename.endswith(\".csv\"):\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                if not df.empty:\n",
    "                    posts_data.append(df)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"Skipping empty or malformed file: {filename}\")\n",
    "        elif filename.startswith(\"reddit_comments_\") and filename.endswith(\".csv\"):\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                if not df.empty:\n",
    "                    comments_data.append(df)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"Skipping empty or malformed file: {filename}\")\n",
    "\n",
    "    all_posts = pd.concat(posts_data, ignore_index=True) if posts_data else pd.DataFrame()\n",
    "    all_comments = pd.concat(comments_data, ignore_index=True) if comments_data else pd.DataFrame()\n",
    "\n",
    "    return all_posts, all_comments\n",
    "# Explanation of relevant columns for prompts and responses:\n",
    "# For posts, 'title' and 'selftext' columns are likely candidates for prompts.\n",
    "# 'selftext' contains the main body of the post, which can be a detailed story prompt.\n",
    "# 'title' can serve as a shorter prompt or a story idea.\n",
    "# For comments, 'comment_body' is the content that will be used as the response to a prompt.\n",
    "# The 'post_id' column in both dataframes will be crucial for linking posts to their respective comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fd98052",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df, comments_df = load_reddit_data(\"../data/raw/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87d35e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all the reddit data\n",
    "posts_df = posts_df.drop_duplicates(subset=['id'])\n",
    "posts_df = posts_df[['id', 'title', 'num_comments', 'score']].rename(columns={'id': 'post_id'})\n",
    "\n",
    "comments_df = comments_df.drop_duplicates(subset=['comment_id'])\n",
    "comments_df = comments_df[['comment_id', 'post_id', 'comment_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f27aafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts DataFrame Head:\n",
      "   post_id                                              title  num_comments  \\\n",
      "0  1lk2jzm  [WP] \"You can't quit! I'm the only one that ca...             7   \n",
      "1  1lk4xdj  [WP] Your greed demon roommate Offered you the...             2   \n",
      "2  1ljtyzu  [WP] A time traveler arrives from the future t...             4   \n",
      "3  1ljpk06  [WP] \"The super soldier serum works perfectly,...            20   \n",
      "4  1lk04u1  [WP] You die unexpectedly. You encounter a dei...             5   \n",
      "\n",
      "   score  \n",
      "0    NaN  \n",
      "1    NaN  \n",
      "2    NaN  \n",
      "3    NaN  \n",
      "4    NaN  \n",
      "\n",
      "Comments DataFrame Head:\n",
      "  comment_id  post_id                                       comment_body\n",
      "0    myzcohn  1lgtwz8  It felt wrong. The air was so dirty, it felt l...\n",
      "1    myyyvg1  1lgtwz8  Okay, that works, which is bad.  \\nI try sever...\n",
      "2    myzb5sw  1lgtwz8  No one had noticed the magic that appeared, to...\n",
      "3    myz6a0x  1lgtwz8  Well, if that works, then-\\n\\nI open a portal ...\n",
      "4    myzcch6  1lgtwz8  I was noting down the most outstanding reading...\n",
      "\n",
      "Posts DataFrame Columns:\n",
      "Index(['post_id', 'title', 'num_comments', 'score'], dtype='object')\n",
      "\n",
      "Comments DataFrame Columns:\n",
      "Index(['comment_id', 'post_id', 'comment_body'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Posts DataFrame Head:\")\n",
    "print(posts_df.head())\n",
    "print(\"\\nComments DataFrame Head:\")\n",
    "print(comments_df.head())\n",
    "\n",
    "print(\"\\nPosts DataFrame Columns:\")\n",
    "print(posts_df.columns)\n",
    "print(\"\\nComments DataFrame Columns:\")\n",
    "print(comments_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b33b5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LangChain and Gemini setup code added.\n"
     ]
    }
   ],
   "source": [
    "# Next, we will proceed with data preprocessing to create prompt-response pairs.\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# --- LangChain and Gemini Setup ---\n",
    "# Replace 'YOUR_API_KEY' with your actual Gemini API key.\n",
    "# You can get an API key from Google AI Studio: https://aistudio.google.com/app/apikey\n",
    "\n",
    "genai.configure(api_key=os.environ.get('GOOGLE_API_KEY'))\n",
    "\n",
    "print(\"\\nLangChain and Gemini setup code added.\")\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\[\\]]', '', text)  # Remove special characters except common punctuation\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = text.strip()  # Remove leading/trailing whitespace\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "def create_prompt_response_pairs(posts_df, comments_df):\n",
    "    # Merge posts and comments on 'id' from posts and 'post_id' from comments\n",
    "    # We want to link each post (prompt) to its comments (responses)\n",
    "    merged_df = pd.merge(posts_df, comments_df, left_on='post_id', right_on='post_id', how='inner')\n",
    "\n",
    "    # Combine title and selftext for the prompt\n",
    "    # Fill NaN in 'selftext' with an empty string so it can be concatenated\n",
    "    merged_df['full_prompt'] = merged_df['title'].fillna('')\n",
    "    merged_df['full_prompt'] = merged_df['full_prompt'].apply(clean_text)\n",
    "    merged_df['comment_body'] = merged_df['comment_body'].apply(clean_text)\n",
    "\n",
    "    # Filter out empty prompts or responses after cleaning\n",
    "    merged_df = merged_df[merged_df['full_prompt'].str.strip() != '']\n",
    "    merged_df = merged_df[merged_df['comment_body'].str.strip() != '']\n",
    "\n",
    "    # For story generation, we can consider each post as a prompt and its comments as potential continuations/responses.\n",
    "    # We'll create a dataset where each row is a (prompt, response) pair.\n",
    "    # A post can have multiple comments, so we'll have multiple entries for a single post.\n",
    "    prompt_response_data = merged_df[['full_prompt', 'comment_body']].rename(columns={'comment_body': 'response'})\n",
    "\n",
    "    return prompt_response_data\n",
    "\n",
    "def split_data(data_df, test_size=0.2, random_state=42):\n",
    "    train_df, val_df = train_test_split(data_df, test_size=test_size, random_state=random_state)\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed19f245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Preprocessing ---\n",
      "Total prompt-response pairs: 2366\n",
      "Prompt-Response Pairs Head:\n",
      "                                         full_prompt  \\\n",
      "0  [wp] you cant quit! im the only one that can b...   \n",
      "1  [wp] you cant quit! im the only one that can b...   \n",
      "2  [wp] you cant quit! im the only one that can b...   \n",
      "3  [wp] you cant quit! im the only one that can b...   \n",
      "4  [wp] you cant quit! im the only one that can b...   \n",
      "\n",
      "                                            response  \n",
      "0  i look at her, as she frowns.  \\nyou would rea...  \n",
      "1  you are nothing but a blight, a simple waste o...  \n",
      "2  i slam the door and stomp off, pleased with my...  \n",
      "3  part 2 \\nafter walking for what seemed like fo...  \n",
      "4  hmm. good story though if i may level some cri...  \n"
     ]
    }
   ],
   "source": [
    "print('\\n--- Data Preprocessing ---')\n",
    "prompt_response_pairs = create_prompt_response_pairs(posts_df, comments_df)\n",
    "print(f'Total prompt-response pairs: {len(prompt_response_pairs)}')\n",
    "print('Prompt-Response Pairs Head:')\n",
    "print(prompt_response_pairs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "432f45aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 1892\n",
      "Validation data size: 474\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = split_data(prompt_response_pairs)\n",
    "print(f'Training data size: {len(train_data)}')\n",
    "print(f'Validation data size: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ba83f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed data saved to train_data.csv and val_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Save processed data for later use\n",
    "train_data.to_csv('../data/cleaned/train_data.csv', index=False)\n",
    "val_data.to_csv('../data/cleaned/val_data.csv', index=False)\n",
    "print('\\nProcessed data saved to train_data.csv and val_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8536d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# --- Fine-tuning Pipeline (Conceptual) ---\n",
    "# Direct fine-tuning of Gemini models is typically done through Google Cloud Vertex AI.\n",
    "# LangChain provides an excellent framework for interacting with LLMs, including Gemini,\n",
    "# and for managing prompts and chains. While LangChain doesn't directly offer a 'fine-tune'\n",
    "# function for Gemini models in the same way you might fine-tune a smaller open-source model,\n",
    "# it can be used to prepare data for fine-tuning on Vertex AI, or to implement few-shot learning\n",
    "# with your processed data.\n",
    "\n",
    "# For the purpose of demonstrating the pipeline, we will show how to set up a LangChain\n",
    "# interaction with a Gemini model for story generation, and how you would conceptually\n",
    "# integrate your processed data.\n",
    "\n",
    "# 1. Initialize the Gemini Model\n",
    "# You can specify the model name, e.g., 'gemini-pro'.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.7)\n",
    "\n",
    "# 2. Define the Prompt Template\n",
    "# This template will guide the model in generating short stories based on your prompts.\n",
    "# We'll use the 'full_prompt' from your Reddit data as the input.\n",
    "story_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"prompt\"],\n",
    "    template=\"\"\"You are a creative short story writer. Based on the following prompt, write a compelling short story:\n",
    "\n",
    "Prompt: {prompt}\n",
    "\n",
    "Story:\"\"\"\n",
    ")\n",
    "\n",
    "# 3. Create an LLM Chain\n",
    "# This chain combines the prompt template with the Gemini LLM.\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 4. Chain\n",
    "story_chain = story_prompt_template | llm | output_parser\n",
    "\n",
    "# --- How to use your processed data for 'training' or few-shot learning ---\n",
    "# Since direct fine-tuning is via Vertex AI, here's how you'd use your data:\n",
    "\n",
    "# Option A: Few-Shot Learning with LangChain (if your data is small enough)\n",
    "# You can create examples from your train_data.csv and include them in your prompt.\n",
    "# This is effective for smaller datasets or for guiding the model's style.\n",
    "# Example (this would be done within a more complex prompt engineering setup):\n",
    "# from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "# from langchain.chains import LLMChain\n",
    "\n",
    "# examples = []\n",
    "# # Load a few examples from your train_data.csv\n",
    "# # For demonstration, let's assume you have a list of dicts like:\n",
    "# # examples = [\n",
    "# #     {\"prompt\": \"A lone astronaut discovers a strange artifact on Mars.\", \"response\": \"The artifact pulsed with an eerie light...\"},\n",
    "# #     # ... more examples\n",
    "# # ]\n",
    "\n",
    "# # example_prompt = PromptTemplate(input_variables=[\"prompt\", \"response\"], template=\"Prompt: {prompt}\\nStory: {response}\")\n",
    "# # few_shot_prompt = FewShotPromptTemplate(\n",
    "# #     examples=examples,\n",
    "# #     example_prompt=example_prompt,\n",
    "# #     prefix=\"Write a short story based on the given prompt. Here are some examples:\",\n",
    "# #     suffix=\"\\nPrompt: {prompt}\\nStory:\",\n",
    "# #     input_variables=[\"prompt\"],\n",
    "# #     example_separator=\"\\n\\n\"\n",
    "# # )\n",
    "\n",
    "# # few_shot_chain = LLMChain(llm=llm, prompt=few_shot_prompt)\n",
    "\n",
    "# Option B: Preparing Data for Vertex AI Fine-tuning\n",
    "# Your `train_data.csv` and `val_data.csv` are already in a suitable format (prompt, response pairs).\n",
    "# You would typically upload these to Google Cloud Storage and then use Vertex AI's API or UI\n",
    "# to initiate a fine-tuning job. The `full_prompt` column would be your input, and `response`\n",
    "# would be your target output for the model to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6be80af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google model direct call for checking\n",
    "llm_google = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "def call_model(prompt):\n",
    "    prompt_text = f\"\"\"You are a creative short story writer. Based on the following prompt, write a compelling short story:\n",
    "\n",
    "    Prompt: {prompt}\n",
    "\n",
    "    Story:\"\"\"\n",
    "    return llm_google.generate_content(prompt_text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1444515f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Story for Prompt: A detective investigates a mysterious disappearance in a foggy, old town. ---\n",
      "The fog was a shroud, clinging to Oakhaven like a damp, mournful spirit. Detective Silas Blackwood pulled his collar higher, the chill seeping into his bones despite the thick wool coat. Oakhaven was a town that time had forgotten, a cluster of crooked houses and cobblestone streets huddled together under a perpetually gray sky. And now, it was a town with a ghost – or rather, the absence of one.\n",
      "\n",
      "Eliza Ainsworth had vanished five days ago. Not a trace, not a whisper. Just… gone.\n",
      "\n",
      "Silas stood outside Eliza’s cottage, a miniature gingerbread house swallowed by overgrown ivy. The front door hung ajar, creaking in the wind like a rusty hinge of fate. Inside, the air was thick with the scent of lavender and dust. Eliza, a renowned herbalist, lived alone, her days dedicated to the whispering secrets of the plants.\n",
      "\n",
      "He surveyed the living room. Orderly, almost unnervingly so. A half-finished embroidery lay on a table, a vibrant fox frozen mid-stride. A teacup sat beside it, stained with a dark, bitter residue. Silas carefully bagged it.\n",
      "\n",
      "Old Mrs. Gable, the town's self-appointed oracle and resident gossip, had told him Eliza kept to herself, distrustful of outsiders. \"She knew too much about herbs and their…properties,\" she’d whispered, her eyes wide with a mixture of fear and fascination. \"Some folks say she could brew potions that could make a person disappear.\"\n",
      "\n",
      "Silas scoffed internally. Superstition was a comfortable blanket in towns like Oakhaven, a way to explain the unexplained. Still, he couldn’t dismiss it entirely. The lack of forced entry suggested Eliza had known her visitor. Trusted them, perhaps.\n",
      "\n",
      "He spent the next few days navigating the fog-choked streets, interviewing the townsfolk. Each conversation was a puzzle piece, slippery and reluctant to fit. They spoke of Eliza with a mixture of awe and suspicion, their stories laced with folklore and half-truths.\n",
      "\n",
      "Then, he found it. Tucked away behind the dusty herbarium in Eliza’s cottage, a hidden compartment. Inside, a small wooden box. And inside the box, a single, dried sprig of wolfsbane.\n",
      "\n",
      "Wolfsbane, also known as aconite, was a potent poison. In small doses, it could numb pain and induce a state of tranquility. In larger doses… well, Mrs. Gable’s whispers of disappearing potions suddenly felt less like folklore.\n",
      "\n",
      "He showed the wolfsbane to Dr. Finch, the town's aging physician. The doctor paled. \"Extremely dangerous,\" he confirmed, his voice trembling slightly. \"Could induce paralysis, even death. But… it could also induce a state resembling death. If administered correctly, a person could appear… gone.\"\n",
      "\n",
      "Suddenly, the fog felt colder, the silence more profound. Silas revisited Eliza's cottage, this time with a new perspective. He noticed a faint, earthy smell in the basement, a cellar crammed with jars of herbs and tinctures. He followed the scent to a corner shrouded in shadow.\n",
      "\n",
      "And there, hidden behind a tapestry of dried flowers, was a freshly dug patch of earth.\n",
      "\n",
      "With a grim determination, Silas began to dig. The soil was loose, damp. His shovel struck something hard. He knelt, brushing away the earth, and his heart plummeted.\n",
      "\n",
      "It was a coffin. Small, crudely made. And inside, lying perfectly still, was Eliza Ainsworth.\n",
      "\n",
      "But she wasn’t dead.\n",
      "\n",
      "Her skin was pale, her breathing shallow, almost imperceptible. But Silas saw the faint flutter of her eyelids, the subtle twitch of her fingers.\n",
      "\n",
      "He gently lifted her out of the coffin, her body as light as a feather. He called for Dr. Finch, who arrived with a bag overflowing with antidotes and a face etched with disbelief.\n",
      "\n",
      "It took hours, but slowly, Eliza began to stir. Her eyes fluttered open, unfocused at first, then slowly regaining their clarity.\n",
      "\n",
      "\"Who… who did this to you?\" Silas asked gently.\n",
      "\n",
      "Eliza’s gaze flickered towards him, fear etched on her face. Then, she whispered a name.\n",
      "\n",
      "\"Mr. Hawthorne… the apothecary…\"\n",
      "\n",
      "Mr. Hawthorne, the man who had always been so helpful, so eager to assist Silas in his investigation. The man who had subtly steered him towards the superstition and folklore.\n",
      "\n",
      "Silas finally understood. Hawthorne, driven by jealousy of Eliza’s knowledge and her superior skills, had poisoned her with wolfsbane, intending to bury her alive and claim her secrets as his own.\n",
      "\n",
      "As Hawthorne was led away, his face contorted with rage and defeat, Silas looked at Eliza, weak but recovering. The fog still clung to Oakhaven, but now, a sliver of sunlight pierced through the gloom. He had brought a ghost back to life, and in doing so, he had finally cleared the fog from Oakhaven's dark heart. The town wouldn't be the same, but maybe, just maybe, it could finally start to heal.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of how you would use the chain for inference after hypothetical fine-tuning or with few-shot:\n",
    "# Example of generating a story using the defined chain\n",
    "sample_prompt = \"A detective investigates a mysterious disappearance in a foggy, old town.\"\n",
    "print(f\"\\n--- Generating Story for Prompt: {sample_prompt} ---\")\n",
    "# For actual generation, you would call:\n",
    "# generated_story = story_chain.invoke({\"prompt\": sample_prompt})\n",
    "generated_story = call_model(sample_prompt).text\n",
    "print(generated_story)\n",
    "# print(\"To generate a story, uncomment the 'story_chain.run' line and ensure GOOGLE_API_KEY is set.\")\n",
    "# print(\"Note: This is a conceptual demonstration. Actual fine-tuning of Gemini models is done via Google Cloud Vertex AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af604e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "\n",
    "# --- Model Testing and Validation (Conceptual) ---\n",
    "# Since direct fine-tuning is handled by Vertex AI, the evaluation metrics\n",
    "# for the fine-tuned model would typically be provided by Vertex AI itself.\n",
    "# However, we can demonstrate how you would conceptually load a model (or use the base model)\n",
    "# and perform inference for validation or testing purposes.\n",
    "\n",
    "# For evaluating a generative model like Gemini for short story generation,\n",
    "# traditional metrics (like accuracy) are not directly applicable.\n",
    "# Instead, human evaluation or metrics like ROUGE, BLEU (for text similarity),\n",
    "# or more advanced metrics that assess coherence, creativity, and relevance\n",
    "# would be used. LangChain can help with setting up the inference pipeline.\n",
    "\n",
    "# 1. Load the (hypothetically) fine-tuned model or use the base model\n",
    "# In a real scenario with Vertex AI, you would deploy your fine-tuned model\n",
    "# and then interact with its endpoint. Here, we continue with the base Gemini model.\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7) # Already initialized above\n",
    "\n",
    "# 2. Generate Inferences on Validation Data\n",
    "# We will take a few samples from the validation set and generate stories.\n",
    "if __name__ == '__main__':\n",
    "    # ... (previous data loading, preprocessing, and fine-tuning conceptual code)\n",
    "\n",
    "    print(\"\\n--- Model Testing and Validation ---\")\n",
    "    # Load validation data (assuming it was saved)\n",
    "    try:\n",
    "        val_data = pd.read_csv(\"val_data.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Validation data (val_data.csv) not found. Please run the data preprocessing step first.\")\n",
    "        val_data = pd.DataFrame()\n",
    "\n",
    "    if not val_data.empty:\n",
    "        print(\"Generating sample stories from validation prompts...\")\n",
    "        sample_val_prompts = val_data[\"full_prompt\"].sample(min(5, len(val_data)), random_state=42).tolist()\n",
    "\n",
    "        for i, prompt in enumerate(sample_val_prompts):\n",
    "            print(f\"\\nSample {i+1} Prompt: {prompt}\")\n",
    "            # In a real scenario, you would call the model here:\n",
    "            # generated_story = story_chain.run(prompt=prompt)\n",
    "            # print(f\"Generated Story: {generated_story}\")\n",
    "            print(\"Generated Story: [Story generation requires GOOGLE_API_KEY and actual model inference.]\")\n",
    "\n",
    "    print(\"\\nConceptual code for model testing and validation added. Actual evaluation would involve human review or advanced NLP metrics.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
